{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.17.0)\n",
      "Collecting tensorflow-io\n",
      "  Downloading tensorflow_io-0.37.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: librosa in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (70.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (3.5.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.37.1 (from tensorflow-io)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.8.1)\n",
      "Requirement already satisfied: namex in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pooch>=1.1->librosa) (4.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: pycparser in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Downloading tensorflow_io-0.37.1-cp312-cp312-macosx_12_0_arm64.whl (31.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/31.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp312-cp312-macosx_12_0_arm64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
      "Successfully installed tensorflow-io-0.37.1 tensorflow-io-gcs-filesystem-0.37.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow tensorflow-io librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commands: ['right' 'eight' 'cat' 'tree' 'backward' 'learn' 'bed' 'happy' 'go' 'dog'\n",
      " 'no' 'wow' 'follow' 'nine' 'left' 'stop' 'three' '_background_noise_'\n",
      " 'sheila' 'one' 'bird' 'zero' 'seven' 'up' 'visual' 'marvin' 'two' 'house'\n",
      " 'down' 'six' 'yes' 'on' 'five' 'forward' 'off' 'four']\n",
      "Spectrogram shape: (128, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to your dataset directory\n",
    "data_dir = '/Users/aanishverma/Downloads/speech_commands_v0.02/'\n",
    "\n",
    "# Filter out non-directory entries\n",
    "commands = np.array([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "print(f\"Commands: {commands}\")\n",
    "\n",
    "# Load the WAV file and convert it into a spectrogram\n",
    "def preprocess_audio(filepath, max_length=16000):\n",
    "    audio, sample_rate = librosa.load(filepath, sr=16000)\n",
    "    \n",
    "    # Ensure consistent length (padding or truncating the audio signal)\n",
    "    if len(audio) > max_length:\n",
    "        audio = audio[:max_length]\n",
    "    else:\n",
    "        audio = np.pad(audio, (0, max_length - len(audio)))\n",
    "    \n",
    "    # Convert to a Mel spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "    \n",
    "    # Add the channel dimension for Conv2D\n",
    "    spectrogram_db = np.expand_dims(spectrogram_db, axis=-1)\n",
    "    \n",
    "    return spectrogram_db\n",
    "\n",
    "\n",
    "# Example to process one file (Make sure the file exists)\n",
    "example_file = os.path.join(data_dir, \"yes\", \"0a7c2a8d_nohash_0.wav\")\n",
    "spectrogram = preprocess_audio(example_file)\n",
    "\n",
    "print(f\"Spectrogram shape: {spectrogram.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert string labels to integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_labels)\n",
    "val_labels_encoded = label_encoder.transform(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 84668\n",
      "Validation samples: 21167\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "def load_dataset(data_dir):\n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load each file and its label (the folder name)\n",
    "    for label in os.listdir(data_dir):\n",
    "        label_dir = os.path.join(data_dir, label)\n",
    "        \n",
    "        # Skip files like .DS_Store and only process directories\n",
    "        if not os.path.isdir(label_dir):\n",
    "            continue\n",
    "        \n",
    "        for file in os.listdir(label_dir):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(label_dir, file)\n",
    "                dataset.append(file_path)\n",
    "                labels.append(label)\n",
    "    \n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "dataset, labels = load_dataset(data_dir)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(dataset, labels, test_size=0.2, stratify=labels, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_audio(filepath):\n",
    "    audio_binary = tf.io.read_file(filepath)\n",
    "    audio, _ = tf.audio.decode_wav(audio_binary)\n",
    "    return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "def get_spectrogram(waveform):\n",
    "    # Transform the waveform into a spectrogram\n",
    "    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    return spectrogram\n",
    "\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    return parts[-2]\n",
    "\n",
    "# Preprocess dataset and return both spectrogram and encoded label\n",
    "def preprocess(file_path, label):\n",
    "    spectrogram = preprocess_audio(file_path)\n",
    "    label_encoded = label_encoder.transform([label])[0]\n",
    "    return spectrogram, label_encoded\n",
    "\n",
    "# Apply the preprocessing function to all the training and validation files\n",
    "train_ds = [preprocess(file, label) for file, label in zip(train_files, train_labels)]\n",
    "val_ds = [preprocess(file, label) for file, label in zip(val_files, val_labels)]\n",
    "\n",
    "# Convert lists into numpy arrays\n",
    "train_ds = np.array([x[0] for x in train_ds]), np.array([x[1] for x in train_ds])\n",
    "val_ds = np.array([x[0] for x in val_ds]), np.array([x[1] for x in val_ds])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11520</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,474,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,644</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)    │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_16 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_17 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11520\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,474,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m)             │         \u001b[38;5;34m4,644\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,498,148</span> (5.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,498,148\u001b[0m (5.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,498,148</span> (5.71 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,498,148\u001b[0m (5.71 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Example CNN model\n",
    "# Example CNN model with adjusted input shape\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(128, 32, 1)),  # Adjusted to 32 time steps\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layers for your commands\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 68ms/step - accuracy: 0.4813 - loss: 2.1606 - val_accuracy: 0.7558 - val_loss: 0.8346\n",
      "Epoch 2/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 61ms/step - accuracy: 0.8139 - loss: 0.6376 - val_accuracy: 0.8009 - val_loss: 0.6830\n",
      "Epoch 3/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 62ms/step - accuracy: 0.8555 - loss: 0.4832 - val_accuracy: 0.8144 - val_loss: 0.6590\n",
      "Epoch 4/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 60ms/step - accuracy: 0.8761 - loss: 0.4003 - val_accuracy: 0.8255 - val_loss: 0.6214\n",
      "Epoch 5/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 61ms/step - accuracy: 0.8993 - loss: 0.3266 - val_accuracy: 0.8173 - val_loss: 0.7117\n",
      "Epoch 6/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 60ms/step - accuracy: 0.9106 - loss: 0.2828 - val_accuracy: 0.8243 - val_loss: 0.6897\n",
      "Epoch 7/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 61ms/step - accuracy: 0.9220 - loss: 0.2506 - val_accuracy: 0.8109 - val_loss: 0.7744\n",
      "Epoch 8/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 61ms/step - accuracy: 0.9299 - loss: 0.2224 - val_accuracy: 0.8286 - val_loss: 0.7475\n",
      "Epoch 9/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 61ms/step - accuracy: 0.9376 - loss: 0.1990 - val_accuracy: 0.8209 - val_loss: 0.8731\n",
      "Epoch 10/10\n",
      "\u001b[1m2646/2646\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 61ms/step - accuracy: 0.9425 - loss: 0.1832 - val_accuracy: 0.8254 - val_loss: 0.8841\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the processed data and encoded labels\n",
    "history = model.fit(train_ds[0], train_ds[1], epochs=10, validation_data=(val_ds[0], val_ds[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
